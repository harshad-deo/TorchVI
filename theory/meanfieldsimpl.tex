\documentclass[10pt]{article}
\usepackage[a4paper, margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage[style=iso]{datetime2}
\usepackage{amsmath}
\usepackage{eufrak}
\usepackage{amssymb}
\usepackage{mathtools}

\newcommand{\defeq}{\vcentcolon=}

\title{Automatic Differentiation Variational Inference - Mean Field Simplification}
\author{Harshad Deo \\ 
  \href{mailto:harshad@moreficent.com}{harshad@moreficent.com} \\ 
  \href{mailto:harshad@simianquant.com}{harshad@simianquant.com}
}
\date{}

\setlength{\parindent}{0cm}

\hypersetup{
  colorlinks,
  citecolor=blue,
  filecolor=blue,
  linkcolor=blue,
  urlcolor=blue
}
  
\setlength{\parskip}{\baselineskip}%
  
\begin{document}
  
\maketitle

This monograph is a follow up to the article introducing Auto Differentiation Variational Inference and develops simplifying
assumptions that allow the technique to be used to train large, complex models over large datasets, for example Bayesian 
Deep Learning.

\section*{Decoupling The Latent Space}

The full rank approximation, developed in the previous monograph, allows all the latent variables ($\theta$) to be coupled
with each other through the correlation matrix of the variational approximation. As a result, the number of parameters
required to model the latent space grows quadratically with the size of the latent space. This makes it intractable for 
use in all but the simplest problems, with a few thousand latent variables. 

The mean field approximation assumes that all the latent variables are independent. Therefore, the number of parameters
required to model the latent space grows linearly with the size of the latent space, making it practical to scale the size
of the model. Formally,

\begin{align*}
  q(\zeta, \phi) \defeq& \mathcal{N}\big(\zeta; \mu, \text{diag}(\exp(\omega) ^2)\big) \\
  =& \prod_{k=1}^K \mathcal{N}(\zeta_k; \mu_k, \exp(\omega_k)^2)
\end{align*}

With this parameterization, the space of the variational parameters is given by:

\begin{equation*}
  \Phi = \{\mu_1, \ldots, \mu_k, \omega_1, \ldots, \omega_k\} = \mathbb{R}^{2K}
\end{equation*}

And the elliptical standardization is given by:

\begin{equation*}
  \eta_k = S_{\phi}(\zeta) = \frac{\zeta_k - \mu_k}{\exp \omega_k}
\end{equation*}

The correlation, if required, now needs to be modelled in explicitly as a function of the latent variables. For example, 
the Lewandowski-Kurowicka-Joe transform \footnote{Lewandowski, D., Kurowicka, D., \& Joe, H. (2009). Generating random correlation matrices based on vines and extended onion method. Journal of multivariate analysis, 100(9), 1989-2001.} can be used to explicitly model a correlation matrix for a subset of the latent
variables. 

\section*{Entropy Simplification}

The contribution of the entropy of the variational approximation to the ELBO can be simplified as:

\begin{align*}
  \frac{1}{2}\ln| \det \Sigma ^ 2| &= \frac{1}{2}\ln |\det \big(\text{diag}(\exp(\omega) ^ 2)\big)|  \\
  &= \frac{1}{2}\ln \prod_{k=1}^{K}\big(\exp(\omega_k)^2\big) \\
  &= \ln \prod_{k=1}^{K}\exp \omega_k \\
  &= \sum_{k=1}^{K}\omega_k
\end{align*}

Therefore the optimization objective can be simplified to:

\begin{equation*}
  \phi^* = \underset{\phi}{\text{argmax}} \, \mathbb{E}_{\mathcal{N}(\eta; 0, 1)}\big[\log p(x, T^{-1}(S_\phi^{-1}(\eta))) + \log |\det J_{T^{-1}}(S_\phi^{-1}(\eta))|\big] + \sum_{k=1}^{K}\omega_k
\end{equation*}

\section*{Joint Probability Simplification}

The joint probability of the data and the parameters, $p\big(x, T^{-1}(S_\phi^{-1}(\eta))\big)$ can be factorized as the product 
of the probability of the data, given the parameters and the prior probability of the parameters. The objective can thus be 
reexpressed as:

\begin{equation*}
  \phi^* = \underset{\phi}{\text{argmax}} \, \mathbb{E}_{\mathcal{N}(\eta; 0, 1)}\big[\log p(x | T^{-1}(S_\phi^{-1}(\eta))) + \log p(T^{-1}(S_\phi^{-1}(\eta))) + \log |\det J_{T^{-1}}(S_\phi^{-1}(\eta))|\big] + \sum_{k=1}^{K}\omega_k
\end{equation*}


Therefore, it is observed that the prior probability acts as a regularizing term on the distribution of the latent space. This gives 
a clear and simple interpretation to standard regularization techniques like $L_1$ and $L_2$ regularization as assigning Laplace
and Gaussian priors respectively. 

\section*{Graph Simplification}

To simplify the expression further, the family of constraint relaxation functions, $T(\theta)$ has to be restricted to those
for whom the Jacobian of the inverse transform is a triangular matrix. This is a benign assumption in practice because it is
true for virtually all of the commonly used transformations. Since the Jacobian of a triangular matrix is the product of 
its diagonal, this assumption allows the computational graph to be factorized into mutually exclusive subgraphs $U_1 \ldots U_M, M \leq K$
such that $U_m, 1 \le m \leq M$ is only dependent on $U_1 \ldots U_{m - 1}$. 

\end{document}
